# python 爬虫知识体系
## 简介
1. 爬虫具体流程  
爬虫代码用到一些 Http 库向指定的服务器偷偷摸摸的发起请求，这个时候爬虫可以假装自己是浏览器（添加一些header信息）模拟浏览器向服务器发送代码，请求数据（html、json、二进制数据），将数据存储在目标位置
2. HOW（怎样爬取数据）
- http请求头（Accept、Host、cookie、User-Agent等）：通过请求头告诉浏览器我们是正规请求（比如设置cookie等）
- 请求头：提交的数据
- 响应头：告诉我们数据以什么形式展现
- 响应体：服务器返回的数据
3. 原理（拦截并修改浏览器与服务器之间的消息，得到自己想要的数据）
- 浏览器会默认读一个系统代理（本机的8888端口）向服务器发送请求
- fiddler一旦打开就会作为系统代理
- 因此浏览器访问服务器前会经过fildler（fildler就可以抓住请求消息）
4. http
- 请求报文：请求行、请求头部、空一行、请求体
- 响应报文：响应状态、响应头、空一行、响应体
5. fiddler使用
- statics:性能统计与分析（测试人员用的较少）
- inspector:检查器，检查请求
- autoresponser：自定义响应内容（配置url以及响应内容即可）
- composer:自己设计请求，帮助发包（类似jmeter进行接口测试）（可直接将左边的请求拖动到composer中）
- filter:过滤器
## fiddler应用
1. 极端测试-网络中断-断点：
- 全局断点：可以进行极端测试如模拟网络中断
- 局部断点：bpu /bpafter
2. 弱网测试
设置rules-perfomance-simulated mode 限制网速（也可以自定义延迟时间）
3. https 
需要安装证书（火狐需要设置证书）
## 插件安装
- willow：以项目的形式管理请求